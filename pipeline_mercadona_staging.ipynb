{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "594d7495-e554-4aed-b0aa-bf3dcfb9f288",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "acf35cac-87c5-4d47-856c-e4f0e6cb6929",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# RAW NOTEBOOK - con función para el orquestador\n",
    "\n",
    "def run_raw(audit_date: str) -> str:\n",
    "    \"\"\"\n",
    "    Ejecuta el scraping de Mercadona para una fecha dada\n",
    "    y guarda los datos en Delta Lake particionados por audit_date.\n",
    "    Devuelve \"OK\" si todo salió bien.\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"=== INICIO RAW - audit_date={audit_date} ===\")\n",
    "\n",
    "    # Carpeta en DBFS para JSONs\n",
    "    dbfs_folder = \"/dbfs/FileStore/json_categorias\"\n",
    "    os.makedirs(dbfs_folder, exist_ok=True)\n",
    "\n",
    "    # Cabeceras para requests\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "                      \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "                      \"Chrome/117.0.0.0 Safari/537.36\"\n",
    "    }\n",
    "\n",
    "    # IDs de categorías a scrapear\n",
    "    all_category_ids = [112, 115, 156, 118, 77, 48,\n",
    "                        86, 99, 897, 226, 78, 159,\n",
    "                        135, 164, 168, 169]\n",
    "\n",
    "    # --- SCRAPING ---\n",
    "    print(\"=== INICIO SCRAPING ===\")\n",
    "    for cat_id in all_category_ids:\n",
    "        url = f\"https://tienda.mercadona.es/api/categories/{cat_id}/\"\n",
    "        try:\n",
    "            r = requests.get(url, headers=headers)\n",
    "            if r.status_code == 200:\n",
    "                productos = r.json()\n",
    "                filename = f\"{dbfs_folder}/categoria_{cat_id}_{audit_date}.json\"\n",
    "                with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "                    json.dump(productos, f, ensure_ascii=False, indent=4)\n",
    "                print(f\"[OK] Guardado JSON categoría {cat_id} para {audit_date}\")\n",
    "            else:\n",
    "                print(f\"[ERROR] {r.status_code} en categoría {cat_id}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] en categoría {cat_id}: {e}\")\n",
    "        time.sleep(0.5)\n",
    "    print(\"=== FIN SCRAPING ===\")\n",
    "\n",
    "    # --- TRANSFORMACIÓN ---\n",
    "    print(\"=== INICIO TRANSFORMACIÓN ===\")\n",
    "    productos_lista = []\n",
    "    for cat_id in all_category_ids:\n",
    "        file_path = f\"/dbfs/FileStore/json_categorias/categoria_{cat_id}_{audit_date}.json\"\n",
    "        try:\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                data = json.load(f)\n",
    "            for cat in data.get(\"categories\", []):\n",
    "                if \"products\" in cat:\n",
    "                    primeros_6 = cat[\"products\"][:6]\n",
    "                    for p in primeros_6:\n",
    "                        price = p.get(\"price_instructions\", {})\n",
    "                        productos_lista.append({\n",
    "                            \"id_producto\": p.get(\"id\", \"\"),\n",
    "                            \"nombre\": p.get(\"display_name\", \"\"),\n",
    "                            \"categoria\": cat.get(\"name\", \"\"),\n",
    "                            \"categoria_id\": cat_id,\n",
    "                            \"packaging\": p.get(\"packaging\", \"\"),\n",
    "                            \"precio_unitario\": price.get(\"unit_price\", None),\n",
    "                            \"precio_pack\": price.get(\"bulk_price\", None),\n",
    "                            \"precio_referencia\": price.get(\"reference_price\", None),\n",
    "                            \"formato_referencia\": price.get(\"reference_format\", \"\"),\n",
    "                            \"precio_anterior\": price.get(\"previous_unit_price\", None),\n",
    "                            \"iva\": price.get(\"iva\", None),\n",
    "                            \"precio_bajado\": price.get(\"price_decreased\", False),\n",
    "                            \"tamano_unidad\": price.get(\"unit_size\", None),\n",
    "                            \"nombre_unidad\": price.get(\"unit_name\", \"\"),\n",
    "                            \"unidades_totales\": price.get(\"total_units\", None),\n",
    "                            \"fecha_no_disponible\": p.get(\"unavailable_from\", None),\n",
    "                            \"audit_date\": audit_date\n",
    "                        })\n",
    "        except FileNotFoundError:\n",
    "            print(f\"[WARN] No se encontró {file_path}\")\n",
    "\n",
    "    df = pd.DataFrame(productos_lista)\n",
    "    print(f\"[OK] Transformación completada con {len(df)} productos\")\n",
    "\n",
    "    # --- CARGA EN DELTA ---\n",
    "    print(\"=== INICIO CARGA EN DELTA LAKE ===\")\n",
    "    df_spark = spark.createDataFrame(df)\n",
    "    delta_path_partitioned = \"/mnt/datalake/tfm/mercadona/raw/\"\n",
    "\n",
    "    df_spark.write.format(\"delta\") \\\n",
    "        .mode(\"append\") \\\n",
    "        .partitionBy(\"audit_date\") \\\n",
    "        .save(delta_path_partitioned)\n",
    "\n",
    "    print(f\"[OK] Datos cargados en Delta Lake particionados por audit_date={audit_date}\")\n",
    "    print(\"=== FIN RAW ===\")\n",
    "\n",
    "    return \"OK\"\n",
    "\n",
    "\n",
    "# --- Entrada desde orquestador ---\n",
    "dbutils.widgets.text(\"run_date\", \"\")\n",
    "audit_date = dbutils.widgets.get(\"run_date\")\n",
    "\n",
    "result = run_raw(audit_date)\n",
    "dbutils.notebook.exit(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b35a59f6-f046-4f57-a4bc-db4fa8f1506a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "pipeline_mercadona_staging",
   "widgets": {
    "run_date": {
     "currentValue": "",
     "nuid": "db7c0ca8-9ea8-40e8-858d-d774f361d463",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "run_date",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "run_date",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}